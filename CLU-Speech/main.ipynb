{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make these environment variables\n",
    "SPEECH_KEY = \"3f1363212b944feb953bfe0038c13d1b\"\n",
    "SPEECH_REGION = \"eastus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH = \"en-US\"\n",
    "HINDI = \"hi-IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_from_microphone(speech_language):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "    speech_config.speech_recognition_language=speech_language\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n",
      "Recognized: Hi, what are you doing?\n"
     ]
    }
   ],
   "source": [
    "recognize_from_microphone(ENGLISH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n",
      "Recognized: हैलो, आप क्या कह रहे हो?\n"
     ]
    }
   ],
   "source": [
    "recognize_from_microphone(HINDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n",
      "Recognized: హాయ్ మీరు ఏమి చేస్తున్నారు.\n"
     ]
    }
   ],
   "source": [
    "recognize_from_microphone(\"te-IN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure.ai.translation.text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/kavyagnihotri/Documents/personal/acads/4-1/chatbot for blindchildren/CLU-Speech/main.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kavyagnihotri/Documents/personal/acads/4-1/chatbot%20for%20blindchildren/CLU-Speech/main.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranslation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TextTranslationClient, TranslatorCredential\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kavyagnihotri/Documents/personal/acads/4-1/chatbot%20for%20blindchildren/CLU-Speech/main.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranslation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m InputTextItem\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kavyagnihotri/Documents/personal/acads/4-1/chatbot%20for%20blindchildren/CLU-Speech/main.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m HttpResponseError\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azure.ai.translation.text'"
     ]
    }
   ],
   "source": [
    "from azure.ai.translation.text import TextTranslationClient, TranslatorCredential\n",
    "from azure.ai.translation.text.models import InputTextItem\n",
    "from azure.core.exceptions import HttpResponseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate_speech_language(speech_language, output_language, translator_key, translator_endpoint, translator_region):\n",
    "    key = translator_key\n",
    "    endpoint = translator_endpoint\n",
    "    region = translator_region\n",
    "\n",
    "    credential = TranslatorCredential(key, region)\n",
    "    text_translator = TextTranslationClient(endpoint=endpoint, credential=credential)\n",
    "\n",
    "    try:\n",
    "        source_language = speech_language\n",
    "        target_language = output_language.split(\"-\")[0]\n",
    "        input_text_elements = [ InputTextItem(text = \"This is a test\") ]\n",
    "\n",
    "        response = text_translator.translate(content = input_text_elements, to = target_language, from_parameter = source_language)\n",
    "        translation = response[0] if response else None\n",
    "\n",
    "        if translation:\n",
    "            for translated_text in translation.translations:\n",
    "                print(f\"Text was translated to: '{translated_text.to}' and the result is: '{translated_text.text}'.\")\n",
    "                return translated_text.text\n",
    "\n",
    "    except HttpResponseError as exception:\n",
    "        print(f\"Error Code: {exception.error.code}\")\n",
    "        print(f\"Message: {exception.error.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_from_microphone_and_translate(speech_language, output_language):\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_translation_config = speechsdk.translation.SpeechTranslationConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "    speech_translation_config.speech_recognition_language=speech_language\n",
    "\n",
    "    target_language=output_language.split(\"-\")[0]\n",
    "    speech_translation_config.add_target_language(target_language)\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    translation_recognizer = speechsdk.translation.TranslationRecognizer(translation_config=speech_translation_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    translation_recognition_result = translation_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if translation_recognition_result.reason == speechsdk.ResultReason.TranslatedSpeech:\n",
    "        print(\"Recognized: {}\".format(translation_recognition_result.text))\n",
    "        print(\"\"\"Translated into '{}': {}\"\"\".format(\n",
    "            target_language, \n",
    "            translation_recognition_result.translations[target_language]))\n",
    "    elif translation_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(translation_recognition_result.no_match_details))\n",
    "    elif translation_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = translation_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n",
      "Recognized: Hi.\n",
      "Translated into 'hi': नमस्ते।\n"
     ]
    }
   ],
   "source": [
    "recognize_from_microphone_and_translate(ENGLISH, HINDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n",
      "Recognized: Hello, tell me about digestion.\n",
      "Translated into 'bn': হ্যালো, হজম সম্পর্কে বলুন।\n"
     ]
    }
   ],
   "source": [
    "recognize_from_microphone_and_translate(ENGLISH, \"bn-IN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_text(text, voice):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "    # The language of the voice that speaks.\n",
    "    speech_config.speech_synthesis_voice_name=voice\n",
    "\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Get text from the console and synthesize to the default speaker.\n",
    "    # print(\"Enter some text that you want to speak >\")\n",
    "    # text = input()\n",
    "\n",
    "    speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "    if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(\"Speech synthesized for text [{}]\".format(text))\n",
    "    elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_synthesis_result.cancellation_details\n",
    "        print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            if cancellation_details.error_details:\n",
    "                print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "                print(\"Did you set the speech resource key and region values?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech synthesized for text [Tell me about ducks]\n"
     ]
    }
   ],
   "source": [
    "text = \"Tell me about ducks\"\n",
    "voice = \"en-US-AriaNeural\"\n",
    "synthesize_text(text, voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech synthesized for text [मुझे बत्तखों के बारे में बताएं]\n"
     ]
    }
   ],
   "source": [
    "text = \"मुझे बत्तखों के बारे में बताएं\"\n",
    "voice = \"hi-IN-SwaraNeural\"\n",
    "synthesize_text(text, voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_UNDERSTANDING_KEY = \"5ee64c53ab2741709cb76937ca7ea778\"\n",
    "LANGUAGE_UNDERSTANDING_SUBSCRIPTION = \"0d62ad5b-0f80-4cde-a734-4f060c0ab9b0\"\n",
    "LANGUAGE_UNDERSTANDING_ENDPOINT = \"https://first-instance-0.cognitiveservices.azure.com/\"\n",
    "LANGUAGE_UNDERSTANDING_REGION = \"eastus\"\n",
    "AZURE_CONVERSATIONS_PROJECT_NAME = \"audio-box\"\n",
    "AZURE_CONVERSATIONS_DEPLOYMENT_NAME = \"initial-deploy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FILE: sample_analyze_conversation_app.py\n",
    "\n",
    "DESCRIPTION:\n",
    "    This sample demonstrates how to analyze user query for intents and entities using\n",
    "    a conversation project with a language parameter.\n",
    "\n",
    "    For more info about how to setup a CLU conversation project, see the README.\n",
    "\n",
    "USAGE:\n",
    "    python sample_analyze_conversation_app.py\n",
    "\n",
    "    Set the environment variables with your own values before running the sample:\n",
    "    1) AZURE_CONVERSATIONS_ENDPOINT                       - endpoint for your CLU resource.\n",
    "    2) AZURE_CONVERSATIONS_KEY                            - API key for your CLU resource.\n",
    "    3) AZURE_CONVERSATIONS_PROJECT_NAME     - project name for your CLU conversations project.\n",
    "    4) AZURE_CONVERSATIONS_DEPLOYMENT_NAME  - deployment name for your CLU conversations project.\n",
    "\"\"\"\n",
    "\n",
    "def sample_analyze_conversation_app(query):\n",
    "    # [START analyze_conversation_app]\n",
    "    # import libraries\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.language.conversations import ConversationAnalysisClient\n",
    "\n",
    "    # get secrets\n",
    "    clu_endpoint = LANGUAGE_UNDERSTANDING_ENDPOINT\n",
    "    clu_key = LANGUAGE_UNDERSTANDING_KEY\n",
    "    project_name = AZURE_CONVERSATIONS_PROJECT_NAME\n",
    "    deployment_name = AZURE_CONVERSATIONS_DEPLOYMENT_NAME\n",
    "\n",
    "    # analyze quey\n",
    "    client = ConversationAnalysisClient(clu_endpoint, AzureKeyCredential(clu_key))\n",
    "    with client:\n",
    "        result = client.analyze_conversation(\n",
    "            task={\n",
    "                \"kind\": \"Conversation\",\n",
    "                \"analysisInput\": {\n",
    "                    \"conversationItem\": {\n",
    "                        \"participantId\": \"1\",\n",
    "                        \"id\": \"1\",\n",
    "                        \"modality\": \"text\",\n",
    "                        \"language\": \"en\",\n",
    "                        \"text\": query\n",
    "                    },\n",
    "                    \"isLoggingEnabled\": False\n",
    "                },\n",
    "                \"parameters\": {\n",
    "                    \"projectName\": project_name,\n",
    "                    \"deploymentName\": deployment_name,\n",
    "                    \"verbose\": True\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # view result\n",
    "    print(f\"query: {result['result']['query']}\")\n",
    "    print(f\"project kind: {result['result']['prediction']['projectKind']}\\n\")\n",
    "\n",
    "    print(f\"top intent: {result['result']['prediction']['topIntent']}\")\n",
    "    print(f\"category: {result['result']['prediction']['intents'][0]['category']}\")\n",
    "    print(f\"confidence score: {result['result']['prediction']['intents'][0]['confidenceScore']}\\n\")\n",
    "\n",
    "    print(\"entities:\")\n",
    "    for entity in result['result']['prediction']['entities']:\n",
    "        print(f\"\\ncategory: {entity['category']}\")\n",
    "        print(f\"text: {entity['text']}\")\n",
    "        print(f\"confidence score: {entity['confidenceScore']}\")\n",
    "        if \"resolutions\" in entity:\n",
    "            print(\"resolutions\")\n",
    "            for resolution in entity['resolutions']:\n",
    "                print(f\"kind: {resolution['resolutionKind']}\")\n",
    "                print(f\"value: {resolution['value']}\")\n",
    "        if \"extraInformation\" in entity:\n",
    "            print(\"extra info\")\n",
    "            for data in entity['extraInformation']:\n",
    "                print(f\"kind: {data['extraInformationKind']}\")\n",
    "                if data['extraInformationKind'] == \"ListKey\":\n",
    "                    print(f\"key: {data['key']}\")\n",
    "                if data['extraInformationKind'] == \"EntitySubtype\":\n",
    "                    print(f\"value: {data['value']}\")\n",
    "\n",
    "    # [END analyze_conversation_app]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: I want to know about digestion\n",
      "project kind: Conversation\n",
      "\n",
      "top intent: TopicIntent\n",
      "category: TopicIntent\n",
      "confidence score: 0.7567197\n",
      "\n",
      "entities:\n",
      "\n",
      "category: Topic\n",
      "text: digestion\n",
      "confidence score: 1\n",
      "extra info\n",
      "kind: EntitySubtype\n",
      "value: general.event\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"I want to know about digestion\"\n",
    "sample_analyze_conversation_app(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: मैं पाचन के बारे में जानना चाहता हूं\n",
      "project kind: Conversation\n",
      "\n",
      "top intent: TopicIntent\n",
      "category: TopicIntent\n",
      "confidence score: 0.97203225\n",
      "\n",
      "entities:\n",
      "\n",
      "category: Topic\n",
      "text: मैं पाचन के बारे\n",
      "confidence score: 1\n",
      "\n",
      "category: Topic\n",
      "text: जानना\n",
      "confidence score: 1\n",
      "\n",
      "category: Topic\n",
      "text: चाहता\n",
      "confidence score: 1\n"
     ]
    }
   ],
   "source": [
    "query = \"मैं पाचन के बारे में जानना चाहता हूं\"\n",
    "sample_analyze_conversation_app(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:  French\n"
     ]
    }
   ],
   "source": [
    "key = LANGUAGE_UNDERSTANDING_KEY\n",
    "endpoint = LANGUAGE_UNDERSTANDING_ENDPOINT\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting the language of text\n",
    "def language_detection_example(client):\n",
    "    try:\n",
    "        documents = [\"Ce document est rédigé en Français.\"]\n",
    "        response = client.detect_language(documents = documents, country_hint = 'us')[0]\n",
    "        print(\"Language: \", response.primary_language.name)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "language_detection_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "KEYWORD_RECOGNITION_MODEL = \"cc120675-5205-4df5-bcaf-1633825dedbb.table\"\n",
    "KEYWORD = \"hey class buddy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_recognize_keyword_locally_from_microphone():\n",
    "    \"\"\"runs keyword spotting locally, with direct access to the result audio\"\"\"\n",
    "\n",
    "    # Creates an instance of a keyword recognition model. Update this to\n",
    "    # point to the location of your keyword recognition model.\n",
    "    model = speechsdk.KeywordRecognitionModel(KEYWORD_RECOGNITION_MODEL)\n",
    "\n",
    "    # The phrase your keyword recognition model triggers on.\n",
    "    keyword = KEYWORD\n",
    "\n",
    "    # Create a local keyword recognizer with the default microphone device for input.\n",
    "    keyword_recognizer = speechsdk.KeywordRecognizer()\n",
    "\n",
    "    done = False\n",
    "    \n",
    "    def recognized_cb(evt):\n",
    "        # Only a keyword phrase is recognized. The result cannot be 'NoMatch'\n",
    "        # and there is no timeout. The recognizer runs until a keyword phrase\n",
    "        # is detected or recognition is canceled (by stop_recognition_async()\n",
    "        # or due to the end of an input file or stream).\n",
    "        result = evt.result\n",
    "        if result.reason == speechsdk.ResultReason.RecognizedKeyword:\n",
    "            print(\"RECOGNIZED KEYWORD: {}\".format(result.text))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "        \n",
    "    def canceled_cb(evt):\n",
    "        result = evt.result\n",
    "        if result.reason == speechsdk.ResultReason.Canceled:\n",
    "            print('CANCELED: {}'.format(result.cancellation_details.reason))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the keyword recognizer.\n",
    "    keyword_recognizer.recognized.connect(recognized_cb)\n",
    "    keyword_recognizer.canceled.connect(canceled_cb)\n",
    "\n",
    "    # Start keyword recognition.\n",
    "    result_future = keyword_recognizer.recognize_once_async(model)\n",
    "    print('Say something starting with \"{}\" followed by whatever you want...'.format(keyword))\n",
    "    result = result_future.get()\n",
    "\n",
    "    # Read result audio (incl. the keyword).\n",
    "    if result.reason == speechsdk.ResultReason.RecognizedKeyword:\n",
    "        time.sleep(2) # give some time so the stream is filled\n",
    "        result_stream = speechsdk.AudioDataStream(result)\n",
    "        result_stream.detach_input() # stop any more data from input getting to the stream\n",
    "\n",
    "        save_future = result_stream.save_to_wav_file_async(\"AudioFromRecognizedKeyword.wav\")\n",
    "        print('Saving file...')\n",
    "        saved = save_future.get()\n",
    "\n",
    "        # If active keyword recognition needs to be stopped before results, it can be done with\n",
    "        \n",
    "        stop_future = keyword_recognizer.stop_recognition_async()\n",
    "        print('Stopping...')\n",
    "        stopped = stop_future.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_recognize_keyword_locally_from_microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
